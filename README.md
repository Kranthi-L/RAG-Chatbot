# ğŸ“˜ CS-297 Phase-1: Comparative Study and Implementation of RAG Chatbots

## ğŸ¯ Goal

The goal of this project is to **reproduce and extend** the ideas from  
**â€œA Comparative Study of Retrieval-Augmented Generation (RAG) Chatbotsâ€**  
by evaluating how different large language models (LLMs) perform when grounded on the same domain corpus using a consistent Retrieval-Augmented Generation (RAG) pipeline.

This project compares **GPT-4o-mini (OpenAI)** and **Claude-3.5 (Anthropic)** on academic textbooks and measures how accurately they answer questions using only retrieved context.

---

## ğŸ’¡ Motivation

General-purpose chatbots like ChatGPT or Claude can answer broad questions but often lack **grounding** to specific sources, which limits factual accuracy in specialized domains like Computer Networking or Computer Architecture.

Retrieval-Augmented Generation (RAG) solves this problem by combining:

- **Retrieval:** fetch relevant information chunks from a trusted corpus.
- **Generation:** let an LLM answer using those retrieved chunks as factual context.

This project explores:

1. How different LLMs perform on **identical retrieval pipelines**.
2. Whether the chatbot can handle **follow-up questions** using conversational context.
3. How the results compare to the findings in the reference paper.

---

## ğŸ§  Overview of Project Phases

| Phase       | Description                                                                                                           | Status       |
| ----------- | --------------------------------------------------------------------------------------------------------------------- | ------------ |
| **Phase 1** | Build a RAG chatbot using course textbooks, compare GPT and Claude using evaluation metrics (ROUGE, BLEU, BERTScore). | âœ… Completed |
| **Phase 2** | Add accessibility features like speech input/output and visual customization for differently-abled learners.          | ğŸš§ Planned   |
| **Phase 3** | Implement adaptive learning capabilities for personalized educational experiences.                                    | ğŸš§ Planned   |

This phase (Phase 1) reproduces the paperâ€™s experimental setup and extends it with persistent sessions, follow-up question reasoning, and a web-based interface.

---

## ğŸ“„ Reference Paper

**Title:** _A Comparative Study of Retrieval-Augmented Generation (RAG) Chatbots_  
**Objective:** Compare multiple LLMs (GPT, Gemini, etc.) under the same RAG pipeline using ROUGE, BLEU, and BERTScore metrics.

**Key Findings from the Paper:**

- Retrieval grounding improved factual accuracy by 15â€“25%.
- GPT-style models showed higher lexical overlap; other models were semantically comparable.
- BERTScore best reflected semantic correctness across all chatbots.

Your implementation follows the same methodology and achieves results within the same metric range.

---

## ğŸš€ Current Progress and Achievements

âœ… Indexed two full textbooks:

- _Computer Networking: A Top-Down Approach_
- _Computer Architecture: A Quantitative Approach_

âœ… Built a complete RAG pipeline (Ingestion â†’ Embedding â†’ Retrieval â†’ Generation).  
âœ… Compared GPT vs Claude quantitatively using ROUGE, BLEU, and BERTScore.  
âœ… Implemented persistent conversation sessions and follow-up question understanding.  
âœ… Created both a Command-Line Interface (CLI) and a Streamlit web app.  
âœ… Achieved metric results consistent with those reported in the paper.

**Evaluation Summary (Networking Dataset):**

| Model       | ROUGE-L | BLEU-4 | BERTScore (F1) | Interpretation                                  |
| ----------- | ------- | ------ | -------------- | ----------------------------------------------- |
| GPT-4o-mini | 0.169   | 2.80   | 0.843          | Strong semantic match; close to ideal phrasing. |
| Claude-3.5  | 0.145   | 2.51   | 0.833          | Semantically accurate; phrasing more narrative. |

---

## ğŸ“‚ Project Structure

rag-chatbot/
â”œâ”€â”€ app_cli.py # CLI chatbot (sessions + follow-ups)
â”œâ”€â”€ app_streamlit.py # Simple single-turn Streamlit UI (baseline)
â”œâ”€â”€ app_web.py # Enhanced Streamlit UI (sessions + follow-ups)
â”œâ”€â”€ ingest.py # PDF ingestion, chunking, embedding, and indexing
â”œâ”€â”€ memory.py # Session persistence (save/load conversations)
â”œâ”€â”€ prompts/
â”‚ â””â”€â”€ qa_system.md # System prompt restricting answers to provided context
â”œâ”€â”€ chroma_db/ # Vector database storing embedded document chunks
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ networking/ # Networking course PDFs
â”‚ â””â”€â”€ architecture/ # Architecture course PDFs
â”œâ”€â”€ eval/
â”‚ â”œâ”€â”€ networking_eval.csv # Question set + ideal answers
â”‚ â”œâ”€â”€ networking_eval_filled.csv # Model answers
â”‚ â”œâ”€â”€ networking_metrics.csv # Evaluation results (ROUGE/BLEU/BERTScore)
â”‚ â”œâ”€â”€ run_batch_networking.py # Script to batch-generate GPT/Claude answers
â”‚ â””â”€â”€ eval_metrics.py # Script to calculate evaluation metrics
â”œâ”€â”€ sessions/ # JSON session logs (persistent conversation memory)
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ .env # API keys for GPT (OpenAI) and Claude (Anthropic)

---

## âš™ï¸ Setup and Installation

# Clone repository

git clone <repo_url>
cd rag-chatbot

# Create and activate a virtual environment

python -m venv .venv
source .venv/bin/activate # (use .venv\Scripts\activate on Windows)

# Install dependencies

pip install -r requirements.txt

# How to Use the Project

ğŸ§± Step 1 â€” Ingest PDFs
python ingest.py

This script:

Extracts text from PDFs under /data/.

Splits text into chunks.

Creates vector embeddings.

Saves them in a local database (/chroma_db/).

ğŸ’¬ Step 2 â€” Chatbot (Command-Line Interface)

Start or resume a session:

python app_cli.py gpt networking --session study1

Examples:

What is TCP congestion control?
How does that help fairness?
What are the drawbacks of congestion control?

Quit with q.
Conversations auto-save in /sessions/<session_id>.json.

ğŸŒ Step 3 â€” Web Interface (Streamlit)
Baseline interface (single-turn chatbot):
streamlit run app_streamlit.py

Enhanced interface (sessions + follow-ups):
streamlit run app_web.py

Features:

Choose backend: GPT, Claude, or Both.

Filter by course: networking, architecture, or all.

Adjust retrieval depth (Top-K) and temperature.

Retrieve and display source filenames and page numbers.

Save and reload conversations by Session ID.

ğŸ§® Step 4 â€” Evaluation Pipeline

1ï¸âƒ£ Generate model responses:

python eval/run_batch_networking.py

2ï¸âƒ£ Compute evaluation metrics:

python eval/eval_metrics.py

Outputs:

networking_eval_filled.csv â€” model-generated answers.

networking_metrics.csv â€” computed ROUGE, BLEU, and BERTScore metrics.

Console summary showing average scores per model.

ğŸ“Š Evaluation Metrics Explained
Metric Purpose Interpretation
ROUGE-1/2/L Word and phrase overlap between model and ideal answers. High values = similar phrasing.
BLEU-1/4 N-gram precision, measures exact phrase matches. High values = closer wording.
BERTScore (F1) Semantic similarity using contextual embeddings. High values = meaning preserved even if rephrased.

In explanatory Q&A, high BERTScore (â‰¥0.8) but moderate ROUGE/BLEU indicates correct meaning with varied wording â€” the expected pattern in RAG chatbot evaluations.

ğŸ“ˆ Results Summary

Both GPT and Claude achieved semantic similarity (BERTScore â‰ˆ 0.83â€“0.84), aligning with the paperâ€™s reported results.

GPT scored slightly higher on ROUGE and BLEU, showing tighter phrasing adherence.

Follow-up question support enables pronoun resolution (â€œthat,â€ â€œit,â€ â€œthisâ€) and true multi-turn reasoning.

Metrics validate that your chatbot performs comparably to the paperâ€™s systems.

ğŸ§  Features Implemented
Feature Description Status
PDF ingestion & vector DB Converts textbooks to searchable embeddings. âœ…
RAG pipeline Retrieve relevant context and generate grounded answers. âœ…
Multi-model comparison GPT vs Claude under identical setup. âœ…
Evaluation metrics Automatic scoring (ROUGE, BLEU, BERTScore). âœ…
Session persistence Save and resume chats by session ID. âœ…
Follow-up understanding Summarize and rewrite follow-up questions. âœ…
Streamlit web UI Interactive, user-friendly interface. âœ…
Accessibility (Speech I/O, font control) Voice input/output for differently-abled users. ğŸš§ Phase 2
Adaptive learning Personalized tutoring and progress tracking. ğŸš§ Phase 3
ğŸ”­ Next Phases
Phase 2 â€” Accessibility & Multimodality

Goal: Make the chatbot inclusive and easy to use for differently-abled users.

ğŸ—£ï¸ Speech-to-Text (STT): Convert spoken questions to text (Whisper).

ğŸ”Š Text-to-Speech (TTS): Read out chatbot responses (pyttsx3 or macOS say).

ğŸ¨ Visual Accessibility: Font resizing, color contrast modes, and high-contrast UI.

â™¿ Keyboard-only navigation and ARIA labels for accessibility compliance.

Phase 3 â€” Adaptive Learning & Analytics

Goal: Turn the chatbot into a personalized learning companion.

ğŸ“ˆ Track user performance across sessions.

ğŸ¯ Identify weak topics and generate targeted follow-up questions.

ğŸ§© Dynamically adjust explanation depth and difficulty.

ğŸ“š Integrate analytics dashboard for instructors.
