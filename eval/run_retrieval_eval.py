"""
Compute retrieval metrics (Recall@k, MRR, nDCG) using relevance labels.
Labels are expected at eval/data/relevance_labels.jsonl generated by build_relevance_labels.py.
"""
import json
import os
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Tuple

from dotenv import load_dotenv

from rag_core import retrieve_docs, RetrieverType

load_dotenv()

BASE_DIR = Path(__file__).resolve().parent
REPO_ROOT = BASE_DIR.parent
DATA_DIR = BASE_DIR / "data"
LABELS_PATH = DATA_DIR / "relevance_labels.jsonl"
OUTPUT_PATH = BASE_DIR / "retrieval_metrics.csv"

COURSE_TO_CSV = {
    "networking": REPO_ROOT / "eval" / "networking_eval.csv",
    "architecture": REPO_ROOT / "eval" / "architecture_eval.csv",
    "machine_learning": REPO_ROOT / "eval" / "machine_learning_eval.csv",
}

KS = [2, 4, 6, 8]


def doc_id_from_meta(meta: Dict, fallback_idx: int) -> str:
    course = meta.get("course", "unknown")
    filename = meta.get("filename", meta.get("source", "unknown"))
    page = meta.get("page", "0")
    chunk_idx = meta.get("chunk_index", fallback_idx)
    return f"{course}|{filename}|{page}|{chunk_idx}"


def load_questions(course: str) -> List[Dict]:
    import csv

    path = COURSE_TO_CSV[course]
    rows = []
    with open(path, newline="", encoding="utf-8", errors="replace") as f:
        reader = csv.DictReader(f)
        for r in reader:
            q = (r.get("question") or "").strip()
            if not q or q.startswith("#"):
                continue
            rows.append({"question": q, "ideal_answer": (r.get("ideal_answer") or "").strip()})
    return rows


def load_labels() -> Dict[Tuple[str, int], Dict[str, int]]:
    """
    Returns mapping (course, question_id) -> {doc_id: label}.
    """
    labels: Dict[Tuple[str, int], Dict[str, int]] = defaultdict(dict)
    if not LABELS_PATH.exists():
        raise FileNotFoundError(f"Missing labels at {LABELS_PATH}. Run build_relevance_labels.py first.")
    with open(LABELS_PATH, "r", encoding="utf-8") as f:
        for line in f:
            rec = json.loads(line)
            key = (rec["course"], int(rec["question_id"]))
            labels[key][rec["doc_id"]] = int(rec["label"])
    return labels


def recall_at_k(retrieved: List[str], relevant: Dict[str, int], k: int) -> float:
    rel_ids = {did for did, lbl in relevant.items() if lbl >= 1}
    if not rel_ids:
        return 0.0
    hits = sum(1 for did in retrieved[:k] if did in rel_ids)
    return hits / len(rel_ids)


def mrr(retrieved: List[str], relevant: Dict[str, int], k: int) -> float:
    rel_ids = {did for did, lbl in relevant.items() if lbl >= 1}
    for idx, did in enumerate(retrieved[:k], 1):
        if did in rel_ids:
            return 1.0 / idx
    return 0.0


def dcg(retrieved: List[str], relevant: Dict[str, int], k: int) -> float:
    import math

    score = 0.0
    for i, did in enumerate(retrieved[:k], 1):
        rel = relevant.get(did, 0)
        score += (2**rel - 1) / math.log2(i + 1)
    return score


def ndcg(retrieved: List[str], relevant: Dict[str, int], k: int) -> float:
    ideal_labels = sorted(relevant.values(), reverse=True)
    ideal = 0.0
    import math

    for i, rel in enumerate(ideal_labels[:k], 1):
        ideal += (2**rel - 1) / math.log2(i + 1)
    if ideal == 0:
        return 0.0
    return dcg(retrieved, relevant, k) / ideal


def evaluate():
    labels = load_labels()
    metrics_rows = []

    for course in COURSE_TO_CSV.keys():
        questions = load_questions(course)
        for retriever in RetrieverType:
            for k in KS:
                recalls = []
                mrrs = []
                ndcgs = []
                for qid, row in enumerate(questions):
                    key = (course, qid)
                    if key not in labels:
                        continue
                    relevant = labels[key]
                    docs = retrieve_docs(
                        query=row["question"],
                        course=course,
                        top_k=k,
                        retriever_type=retriever,
                        learner_level=None,
                    )
                    retrieved_ids = [doc_id_from_meta(d.metadata, idx) for idx, d in enumerate(docs)]
                    recalls.append(recall_at_k(retrieved_ids, relevant, k))
                    mrrs.append(mrr(retrieved_ids, relevant, k))
                    ndcgs.append(ndcg(retrieved_ids, relevant, k))

                if recalls:
                    metrics_rows.append({
                        "course": course,
                        "retriever_type": retriever.value,
                        "k": k,
                        "recall_at_k": sum(recalls) / len(recalls),
                        "mrr": sum(mrrs) / len(mrrs),
                        "ndcg": sum(ndcgs) / len(ndcgs),
                        "num_questions": len(recalls),
                    })

    import csv

    with open(OUTPUT_PATH, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=["course", "retriever_type", "k", "recall_at_k", "mrr", "ndcg", "num_questions"])
        writer.writeheader()
        for row in metrics_rows:
            writer.writerow(row)
    print(f"Wrote metrics to {OUTPUT_PATH}")


if __name__ == "__main__":
    evaluate()
